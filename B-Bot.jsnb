{
  "metadata": {
    "name": "New JSNB",
    "language_info": {
      "name": "JavaScipt",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "<div style=\"text-align:center;background-color:#fede02;color:#555;margin:-10px;margin-left:-20px;margin-right:-20px;\">\n        <br><h1  style=\"color:#555\">B-Bot: Your Emotion-Aware AI Companion</h1>\n<p class=\"site-masthead__description mb-0\" style=\"color:#555; font-size: 20px\">\n        Unlocking Human Emotion Through AI\n    </p><br></div>",
      "status": "",
      "output": "<div style=\"text-align:center;background-color:#fede02;color:#555;margin:-10px;margin-left:-20px;margin-right:-20px;\">\n        <br><h1 style=\"color:#555\">B-Bot: Your Emotion-Aware AI Companion</h1>\n<p class=\"site-masthead__description mb-0\" style=\"color:#555; font-size: 20px\">\n        Unlocking Human Emotion Through AI\n    </p><br></div>",
      "type": "html"
    },
    {
      "code": "//>md\n\n# System Architecture Overview <br> <br>\n\n![System Architecture Overview](https://res.cloudinary.com/gauravops/image/upload/v1748619924/architecture_xgvzsd.jpg)",
      "status": "",
      "output": "<h1>System Architecture Overview <br> <br></h1>\n<p><img src=\"https://res.cloudinary.com/gauravops/image/upload/v1748619924/architecture_xgvzsd.jpg\" alt=\"System Architecture Overview\"></p>\n",
      "type": "html"
    },
    {
      "code": "# Prerequisite\n\n| Item                | Description                                                                                                      |\n| ------------------- | ---------------------------------------------------------------------------------------------------------------- |\n| **OpenAI API Key**  | Get your API key [here](https://platform.openai.com/settings/organization/api-keys).                             |\n| **Groq API Key**    | Get your API key [here](https://console.groq.com/keys).                                                          |\n| **Web LLM**         | Reference: [Web LLM GitHub](https://github.com/mlc-ai/web-llm) <br> ⚠️ First load: 3–4 min, Subsequent: ~25 sec. |\n| **Notebook Access** | Click the red button at the top-right corner to take the notebook out of sandbox. ",
      "status": "",
      "output": "<h1>Prerequisite</h1>\n<table>\n<thead>\n<tr>\n<th>Item</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>OpenAI API Key</strong></td>\n<td>Get your API key <a href=\"https://platform.openai.com/settings/organization/api-keys\">here</a>.</td>\n</tr>\n<tr>\n<td><strong>Groq API Key</strong></td>\n<td>Get your API key <a href=\"https://console.groq.com/keys\">here</a>.</td>\n</tr>\n<tr>\n<td><strong>Web LLM</strong></td>\n<td>Reference: <a href=\"https://github.com/mlc-ai/web-llm\">Web LLM GitHub</a> <br> ⚠️ First load: 3–4 min, Subsequent: ~25 sec.</td>\n</tr>\n<tr>\n<td><strong>Notebook Access</strong></td>\n<td>Click the red button at the top-right corner to take the notebook out of sandbox.</td>\n</tr>\n</tbody></table>\n",
      "type": "html"
    },
    {
      "code": "# 🎭 B-Bot: Your Emotion-Aware AI Companion\n\n**B-Bot** B-Bot is a browser-based AI companion that can see your face, sense your emotion, and respond with empathy — in real-time. It combines cutting-edge generative AI tools and browser APIs to create an emotionally intelligent, hands-free conversation experience.\n\n---\n\n## 🚀 What It Does\n\n| 🔧 Feature                       | 💡 Description                                                                                        |\n| -------------------------------- | ----------------------------------------------------------------------------------------------------- |\n| 📸 **Face Snapshot**             | Captures an image from your webcam in real time.                                                      |\n| 🧠 **Emotion Detection (Groq)**  | Uses Groq’s **LLaMA-4** model to interpret your facial expression and describe your emotion in words. |\n| 💬 **Empathetic Reply (WebLLM)** | Local LLM generates a natural, kind response based on how you're feeling — like a caring friend.      |\n| 🔊 **Text-to-Speech (OpenAI)**   | Speaks the AI's reply using OpenAI’s TTS API, simulating a real conversation.                         |\n| 🗣️ **Voice Reply (STT)**         | Optional voice input lets you talk back — or just let the cycle continue with updated emotions.       |\n\n---\n\n## 🛠️ Tools & Technologies Used\n\n| 🧩 Category           | ⚙️ Tools & Technologies                                                                                                                        |\n| --------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| 📒 **Notebook**       | [Scribbler Notebook](https://scribble.world/)                                                                                                  |\n| 🌐 **Frontend**       | HTML, CSS, JavaScript                                                                                                                          |\n| 🧠 **AI Models**      | - [Groq LLaMA-4](https://groq.com/) for emotion interpretation <br> - WebLLM (runs locally using WebGPU)                                       |\n| 🔊 **Voice Features** | - [OpenAI TTS](https://platform.openai.com/docs/guides/text-to-speech) for speaking responses <br> - Browser Audio APIs for speech recognition |\n| 🖥️ **Browser APIs**   | - Webcam via MediaDevices API <br> - Audio capture via Web Speech API.                                                                         |\n\n---\n\n---\n\n## 🌟 Experience\n\n> **Imagine this**: You're having a tough day. You open your browser.  \n> B-Bot sees you, understands your emotion, and gently speaks to you —  \n> just like a friend who really *gets* how you feel. 💛\n",
      "status": "",
      "output": "<h1>🎭 B-Bot: Your Emotion-Aware AI Companion</h1>\n<p><strong>B-Bot</strong> B-Bot is a browser-based AI companion that can see your face, sense your emotion, and respond with empathy — in real-time. It combines cutting-edge generative AI tools and browser APIs to create an emotionally intelligent, hands-free conversation experience.</p>\n<hr>\n<h2>🚀 What It Does</h2>\n<table>\n<thead>\n<tr>\n<th>🔧 Feature</th>\n<th>💡 Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>📸 <strong>Face Snapshot</strong></td>\n<td>Captures an image from your webcam in real time.</td>\n</tr>\n<tr>\n<td>🧠 <strong>Emotion Detection (Groq)</strong></td>\n<td>Uses Groq’s <strong>LLaMA-4</strong> model to interpret your facial expression and describe your emotion in words.</td>\n</tr>\n<tr>\n<td>💬 <strong>Empathetic Reply (WebLLM)</strong></td>\n<td>Local LLM generates a natural, kind response based on how you're feeling — like a caring friend.</td>\n</tr>\n<tr>\n<td>🔊 <strong>Text-to-Speech (OpenAI)</strong></td>\n<td>Speaks the AI's reply using OpenAI’s TTS API, simulating a real conversation.</td>\n</tr>\n<tr>\n<td>🗣️ <strong>Voice Reply (STT)</strong></td>\n<td>Optional voice input lets you talk back — or just let the cycle continue with updated emotions.</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>🛠️ Tools &amp; Technologies Used</h2>\n<table>\n<thead>\n<tr>\n<th>🧩 Category</th>\n<th>⚙️ Tools &amp; Technologies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>📒 <strong>Notebook</strong></td>\n<td><a href=\"https://scribble.world/\">Scribbler Notebook</a></td>\n</tr>\n<tr>\n<td>🌐 <strong>Frontend</strong></td>\n<td>HTML, CSS, JavaScript</td>\n</tr>\n<tr>\n<td>🧠 <strong>AI Models</strong></td>\n<td>- <a href=\"https://groq.com/\">Groq LLaMA-4</a> for emotion interpretation <br> - WebLLM (runs locally using WebGPU)</td>\n</tr>\n<tr>\n<td>🔊 <strong>Voice Features</strong></td>\n<td>- <a href=\"https://platform.openai.com/docs/guides/text-to-speech\">OpenAI TTS</a> for speaking responses <br> - Browser Audio APIs for speech recognition</td>\n</tr>\n<tr>\n<td>🖥️ <strong>Browser APIs</strong></td>\n<td>- Webcam via MediaDevices API <br> - Audio capture via Web Speech API.</td>\n</tr>\n</tbody></table>\n<hr>\n<hr>\n<h2>🌟 Experience</h2>\n<blockquote>\n<p><strong>Imagine this</strong>: You're having a tough day. You open your browser.<br>B-Bot sees you, understands your emotion, and gently speaks to you —<br>just like a friend who really <em>gets</em> how you feel. 💛</p>\n</blockquote>\n",
      "type": "html"
    },
    {
      "code": "//>html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Camera Capture Demo</title>\n</head>\n<body>\n    <h1>Access Camera and Capture Photo</h1>\n    <video id=\"camera\" autoplay></video>\n    <button id=\"capture-btn\">Capture Photo</button>\n    <canvas id=\"photo\" style=\"display:none;\"></canvas>\n\n</body>\n</html>",
      "status": "[1]<br><span style=\"font-size:8px\">1ms<span></span></span>",
      "output": "\n\n\n\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Camera Capture Demo</title>\n\n\n    <h1>Access Camera and Capture Photo</h1>\n    <video id=\"camera\" autoplay=\"\"></video>\n    <button id=\"capture-btn\">Capture Photo</button>\n    <canvas id=\"photo\" style=\"display:none;\"></canvas>\n\n\n <br>",
      "type": "code"
    },
    {
      "code": "//>md\n#### Explanation:\n- `video`, `canvas`, `captureBtn`, and `context` are DOM elements used to show the webcam stream, draw the image, and trigger the capture.\n- `navigator.mediaDevices.getUserMedia({ video: true })` requests access to the user's webcam and displays the live video feed.\n- When the \"Capture\" button is clicked, the current video frame is drawn onto the canvas and converted into a base64-encoded JPEG image.\n- The image is sent to the GROQ API using `fetch`, along with a system prompt and a user query.\n- `GROQ_SYSTEM_PROMPT` instructs the LLM to analyze facial expressions.\n- The API request sends both the prompt and the image to the LLaMA-4 vision model hosted by GROQ.\n- The emotional description response is saved in `window.groq_response` for further use (e.g., display, voice output, or further LLM chaining).\n",
      "status": "",
      "output": "<h4>Explanation:</h4>\n<ul>\n<li><code>video</code>, <code>canvas</code>, <code>captureBtn</code>, and <code>context</code> are DOM elements used to show the webcam stream, draw the image, and trigger the capture.</li>\n<li><code>navigator.mediaDevices.getUserMedia({ video: true })</code> requests access to the user's webcam and displays the live video feed.</li>\n<li>When the \"Capture\" button is clicked, the current video frame is drawn onto the canvas and converted into a base64-encoded JPEG image.</li>\n<li>The image is sent to the GROQ API using <code>fetch</code>, along with a system prompt and a user query.</li>\n<li><code>GROQ_SYSTEM_PROMPT</code> instructs the LLM to analyze facial expressions.</li>\n<li>The API request sends both the prompt and the image to the LLaMA-4 vision model hosted by GROQ.</li>\n<li>The emotional description response is saved in <code>window.groq_response</code> for further use (e.g., display, voice output, or further LLM chaining).</li>\n</ul>\n",
      "type": "html"
    },
    {
      "code": "// Access the DOM elements\nconst video = document.getElementById('camera');\nconst canvas = document.getElementById('photo');\nconst captureBtn = document.getElementById('capture-btn');\nconst context = canvas.getContext('2d');\n\n// Request access to the user's camera\nnavigator.mediaDevices.getUserMedia({ video: true })\n    .then((stream) => {\n        video.srcObject = stream;\n    })\n    .catch((error) => {\n        scrib.error(\"Error accessing the camera:\", error);\n    });\n\n// Capture photo on button click\ncaptureBtn.addEventListener('click', () => {\n    canvas.width = video.videoWidth;\n    canvas.height = video.videoHeight;\n    context.drawImage(video, 0, 0, canvas.width, canvas.height);\n    canvas.style.display = 'block';\n\n    const imageDataURL = canvas.toDataURL('image/jpeg');\n    const base64Image = imageDataURL.split(',')[1];\n\n    const query = \"can you tell me what this image is about?\";\n    analyzeImageWithLLM(base64Image, query);\n});\n\nGROQ_SYSTEM_PROMPT = `\nYou are an expert in analyzing facial expressions to understand human emotions.\n\nYou are given a photo of a person's face. Your job is to analyze the facial features — such as the eyes, mouth, and eyebrows — and determine the person's emotional state **from a first-person perspective**, as if they are describing how they feel.\n\nDo **not** use phrases like \"The person appears to be...\" or \"He/She looks like...\".\n\nInstead, always write from the perspective of the person in the image. Begin the description with **\"My\"**, as if they are expressing their own feelings.\n`\nasync function analyzeImageWithLLM(base64Image, query) {\n  \t// API key and endpoint configuration\n    const apiKey = scrib.getSecret(\"GROQ_API_KEY\");;\n    const apiEndpoint = 'https://api.groq.com/openai/v1/chat/completions';\n    const modelId = \"meta-llama/llama-4-scout-17b-16e-instruct\";\n\n  \t// Headers for the API request\n    const headers = {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n    };\n\n    // Request body including the system prompt, user query, and image data\n    const body = {\n        model: modelId,\n        messages: [\n            {\n                role: \"system\",\n                content: GROQ_SYSTEM_PROMPT\n            },\n            {\n                role: \"user\",\n                content: [\n                    { type: \"text\", text: query },\n                    {\n                        type: \"image_url\",\n                        image_url: {\n                            url: `data:image/jpeg;base64,${base64Image}`\n                        }\n                    }\n                ]\n            }\n        ]\n    };\n  \n    const response = await fetch(apiEndpoint, {\n            method: 'POST',\n            headers: headers,\n            body: JSON.stringify(body)\n        });\n\n    result = await response.json()\n  \n\twindow.groq_response = result.choices[0].message.content;\n}",
      "status": "[2]<br><span style=\"font-size:8px\">1ms<span></span></span>",
      "output": "\nYou are an expert in analyzing facial expressions to understand human emotions.\n\nYou are given a photo of a person's face. Your job is to analyze the facial features — such as the eyes, mouth, and eyebrows — and determine the person's emotional state **from a first-person perspective**, as if they are describing how they feel.\n\nDo **not** use phrases like \"The person appears to be...\" or \"He/She looks like...\".\n\nInstead, always write from the perspective of the person in the image. Begin the description with **\"My\"**, as if they are expressing their own feelings.\n <br>",
      "type": "code"
    },
    {
      "code": "#### Explanation:\n- Dynamically loads WebLLM from CDN to run LLM in-browser.\n- Displays real-time model loading progress using a progress bar and percentage.\n- Initializes the selected LLaMA 3.2 model with `CreateMLCEngine`.\n- Stores the loaded model in `window.engine` for generating responses.\n",
      "status": "",
      "output": "<h4>Explanation:</h4>\n<ul>\n<li>Dynamically loads WebLLM from CDN to run LLM in-browser.</li>\n<li>Displays real-time model loading progress using a progress bar and percentage.</li>\n<li>Initializes the selected LLaMA 3.2 model with <code>CreateMLCEngine</code>.</li>\n<li>Stores the loaded model in <code>window.engine</code> for generating responses.</li>\n</ul>\n",
      "type": "html"
    },
    {
      "code": "webllm = await import(\"https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.72/lib/index.min.js\");",
      "status": "[3]<br><span style=\"font-size:8px\">338ms<span></span></span>",
      "output": "<p class=\"red\">Error accessing the camera:</p> <br>",
      "type": "code"
    },
    {
      "code": "//>html\n<h2>Loading Progress</h2>\n<progress id=\"loading-progress-bar\" value=\"0\" max=\"1\"></progress>\n<span id=\"loading-progress-text\">0%</span>",
      "status": "[4]<br><span style=\"font-size:8px\">0ms<span></span></span>",
      "output": "\n<h2>Loading Progress</h2>\n<progress id=\"loading-progress-bar\" value=\"0\" max=\"1\"></progress>\n<span id=\"loading-progress-text\">Initializing : 0%</span> <br>",
      "type": "code"
    },
    {
      "code": "const progressBar = document.getElementById('loading-progress-bar');\nconst progressText = document.getElementById('loading-progress-text');\nconst initProgressCallback = (initProgress) => {\n  const progressValue = initProgress.progress;\n  progressBar.value = progressValue;\n  progressText.textContent = `${Math.round(progressValue * 100)}%`;\n}\n\nconst selectedModel = \"Llama-3.2-3B-Instruct-q4f16_1-MLC\";\nprogressText.textContent = \"Initializing : 0%\"\nconst engine = await webllm.CreateMLCEngine(\n  selectedModel,\n  { initProgressCallback: initProgressCallback }, // engineConfig\n);\n\nwindow.engine=engine;",
      "status": "[-]",
      "output": "<p class=\"error\">Failed to read the 'caches' property from 'Window': Cache storage is disabled because the context is sandboxed and lacks the 'allow-same-origin' flag.</p>",
      "type": "code"
    },
    {
      "code": "//>md\n#### Explanation:\n- `Webllm_SYSTEM_PROMPTS`: Instructs WebLLM to act like a kind, friendly companion, responding empathetically to a user's emotional description.\n- `webllmText()`: Sends the Groq-generated emotion text to WebLLM and stores its friendly response.\n- `sendToOpenAITTS()`: Sends the WebLLM response to OpenAI's TTS API to generate spoken audio.\n- Plays the returned audio using the browser and enables a stop button.\n- `stopAudio()`: Allows the user to manually stop the audio playback.\n",
      "status": "",
      "output": "<h4>Explanation:</h4>\n<ul>\n<li><code>Webllm_SYSTEM_PROMPTS</code>: Instructs WebLLM to act like a kind, friendly companion, responding empathetically to a user's emotional description.</li>\n<li><code>webllmText()</code>: Sends the Groq-generated emotion text to WebLLM and stores its friendly response.</li>\n<li><code>sendToOpenAITTS()</code>: Sends the WebLLM response to OpenAI's TTS API to generate spoken audio.</li>\n<li>Plays the returned audio using the browser and enables a stop button.</li>\n<li><code>stopAudio()</code>: Allows the user to manually stop the audio playback.</li>\n</ul>\n",
      "type": "html"
    },
    {
      "code": "const Webllm_SYSTEM_PROMPTS = `\nYou are a warm and supportive friend.\n\nYou'll receive a short, first-person description of how someone feels, based on their facial expressions and body language. Your job is to respond like a close friend would — casually, kindly, and with good vibes.\n\nReflect back what they’re feeling with empathy, using friendly and relaxed language. Avoid being too formal or clinical. Instead, talk like you’re chatting with your best friend who just told you how they’re feeling in a selfie or video.\n\nSpeak in 2–4 sentences. You can use contractions, emojis (like 🙂 or 😄), or playful words — just keep it genuine.\n\nExample:\n\nInput:  \nMy eyes are sparkling with excitement as I flash a bright smile, feeling carefree and joyful.\n\nResponse:  \nWow, you look *so* happy! 😄 I can totally feel that carefree energy — like you're having a blast just being in the moment. Honestly, it’s giving pure joy vibes — love to see it!\n`\nasync function webllmText(query, SYSTEM_PROMPT) {\n    const messages = [\n        { role: \"system\", content: SYSTEM_PROMPT },\n        { role: \"user\", content: query },\n    ]\n\n    // Chunks is an AsyncGenerator object\n    const chunks = await engine.chat.completions.create({\n        messages,\n        temperature: 1,\n    });\n\n    window.responseText = chunks.choices[0].message.content\n    sendToOpenAITTS(responseText)\n}\n\nwebllmText(groq_response, Webllm_SYSTEM_PROMPTS)\n\nlet audio;\nlet currentAudioUrl = null\n\nlet stopBtn = document.getElementById(\"stop-btn\");\nstopBtn.disabled = true;  // Disable stop button initially since no audio is playing\n\nasync function sendToOpenAITTS(responseText) {\n    scrib.show(responseText)\n    const apiKey = scrib.getSecret(\"OPENAI_API_KEY\");; // \n    const url = 'https://api.openai.com/v1/audio/speech';\n\n    const headers = {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n    };\n\n    const body = JSON.stringify({\n        model: 'tts-1', // You can choose other models like tts-1-hd\n        input: responseText,\n        voice: 'alloy' // You can choose other voices like: echo, fable, onyx, nova, shimmer\n    });\n\n    try {\n        scrib.show('Sending text to OpenAI TTS...');\n        const response = await fetch(url, {\n            method: 'POST',\n            headers: headers,\n            body: body\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            scrib.error(`OpenAI API Error: ${response.status} ${response.statusText}`, errorData);\n            throw new Error(`OpenAI API Error: ${response.status} ${response.statusText}`);\n        }\n\n        scrib.show('Received audio response from OpenAI TTS.');\n        const audioBlob = await response.blob();\n\t  \n        if (currentAudioUrl) {\n            URL.revokeObjectURL(currentAudioUrl);\n        }\n\t  \n        currentAudioUrl = URL.createObjectURL(audioBlob);\n\t  \n        audio = new Audio(currentAudioUrl);\n        audio.play();\n\t  \tscrib.show('Playing audio...');\n\t  \n        stopBtn.disabled = false;     \n\n        audio.onended = () => {\n            scrib.show('Audio playback finished.');\n            URL.revokeObjectURL(currentAudioUrl); // Clean up the object URL\n        };\n\n        audio.onerror = (e) => {\n            scrib.error('Error playing audio:', e);\n            URL.revokeObjectURL(currentAudioUrl); // Clean up the object URL\n        };\n\n    } catch (error) {\n        scrib.error('Error sending request to OpenAI TTS:', error);\n    }\n}\n\nfunction stopAudio() {\n    if (audio) {\n        audio.pause();\n        audio.currentTime = 0;\n        stopBtn.disabled = true;\n        scrib.show(\"Audio stopped.\");\n    }\n    audio = new Audio(currentAudioUrl);\n}",
      "status": "[-]",
      "output": "<p class=\"error\">groq_response is not defined</p>",
      "type": "code"
    },
    {
      "code": "#### Explanation:\n- Checks if the browser supports the Web Speech API.\n- Initializes `webkitSpeechRecognition` for live voice transcription.\n- Captures and stores user speech in `window.transcript` as text.\n- Automatically stops recording after 5 seconds of silence.\n- Extracts and pushes user input into `window.my_mood` with emotion context.\n- Sends the transcript and mood data to WebLLM for empathetic response generation.\n- Dynamically creates a \"Start Recording\" button for interaction.\n",
      "status": "",
      "output": "<h4>Explanation:</h4>\n<ul>\n<li>Checks if the browser supports the Web Speech API.</li>\n<li>Initializes <code>webkitSpeechRecognition</code> for live voice transcription.</li>\n<li>Captures and stores user speech in <code>window.transcript</code> as text.</li>\n<li>Automatically stops recording after 5 seconds of silence.</li>\n<li>Extracts and pushes user input into <code>window.my_mood</code> with emotion context.</li>\n<li>Sends the transcript and mood data to WebLLM for empathetic response generation.</li>\n<li>Dynamically creates a \"Start Recording\" button for interaction.</li>\n</ul>\n",
      "type": "html"
    },
    {
      "code": "// Check if the browser supports the Web Speech API\nif (!('webkitSpeechRecognition' in window)) {\n    alert(\"Your browser doesn't support the Web Speech API. Please use Chrome or Edge.\");\n} else {\n    // Define the helper functions first\n    function webllmTextCalled(transcript, USER_MOOD_SYSTEM_PROMPT) {\n        webllmText(transcript, USER_MOOD_SYSTEM_PROMPT)\n    }\n\n    // Create a new instance of SpeechRecognition\n    const recognition = new webkitSpeechRecognition();\n\n    // Initialize mood tracking\n    window.my_mood = [];\n\n    // Define the system prompt\n    window.USER_MOOD_SYSTEM_PROMPT = `\n    You are a helpfull AI Assistant who is specialized in understanding human Emotions and Feelings.\n\n    Your job is to analyse the my_mood and resolve the user question.\n\n    my_mood break down:\n    my_mood contains the user_emotion please understand it before moving to the next step. then it also contains user_said which express how the user is felling that emotions.\n\n    my_mood: ${window.my_mood}\n\n    Example:\n    {{user_emotion: The person in the image appears to be feeling Neutral. His facial expression does not convey strong emotions like happiness, sadness, anger, or fear. His eyebrows are \t\tnot furrowed or raised, and his mouth is slightly closed, indicating a calm demeanor., \n    user_said: Yeah, I'm feeling neutral because it just felt like a normal day—it wasn't very productive for me.}}\n    \n    assistant: That makes a lot of sense. Not every day has to be super productive—sometimes those slower, more neutral days are necessary to recharge. If you're feeling a bit stuck or want \t  to ease into something more productive tomorrow, I can help you plan something light and manageable. Or we can just talk about anything else on your mind.\n\t`;\n\n    // Set properties\n    recognition.continuous = true;\n    recognition.interimResults = true;\n    recognition.lang = 'en-US';\n\n    // Create start button dynamically\n    const startButton = document.createElement('button');\n    startButton.textContent = 'Start Recording';\n    startButton.id = 'start-btn';\n\n    // Add button to the document\n    document.body.appendChild(startButton);\n\n    let isRecording = false;\n    let silenceTimer = null;\n    let lastSpeechTime = null;\n\n    function stopRecording() {\n        if (isRecording) {\n            recognition.stop();\n            isRecording = false;\n            startButton.disabled = false;\n\n            // Clear the silence timer\n            if (silenceTimer) {\n                clearTimeout(silenceTimer);\n                silenceTimer = null;\n            }\n\n            // Call your functions with the final transcript\n            if (window.transcript) {\n                // Update my_mood with the latest transcript\n                window.my_mood.push({\n                    user_emotion: \"Processing...\",\n                    user_said: window.transcript\n                });\n\n                // Call the functions directly\n                webllmTextCalled(window.transcript, window.USER_MOOD_SYSTEM_PROMPT);\n                USER_MOOD_SYSTEM(window.USER_MOOD_SYSTEM_PROMPT);\n\n                // Log to confirm the calls\n                scrib.show(\"Functions called with transcript:\", window.transcript);\n            }\n        }\n    }\n\n    // Start button click event\n    startButton.onclick = () => {\n        if (!isRecording) {\n            window.transcript = ''; // Reset transcript\n            recognition.start();\n            isRecording = true;\n            startButton.disabled = true;\n            lastSpeechTime = Date.now();\n        }\n    };\n\n    // Handle the result when speech is detected\n    recognition.onresult = (event) => {\n        const results = Array.from(event.results);\n        window.transcript = results.map(result => result[0].transcript).join(' ');\n        scrib.show(\"Current transcript: \" + window.transcript);\n\n        // Reset the silence timer\n        lastSpeechTime = Date.now();\n        if (silenceTimer) {\n            clearTimeout(silenceTimer);\n        }\n\n        // Set new silence timer\n        silenceTimer = setTimeout(() => {\n            if (Date.now() - lastSpeechTime >= 5000) {\n                stopRecording();\n            }\n        }, 5000);\n    };\n\n    // Handle speech end\n    recognition.onspeechend = () => {\n        // Start the silence timer\n        if (!silenceTimer) {\n            silenceTimer = setTimeout(() => {\n                if (Date.now() - lastSpeechTime >= 5000) {\n                    stopRecording();\n                }\n            }, 5000);\n        }\n    };\n\n    // Handle errors\n    recognition.onerror = (event) => {\n        scrib.error('Speech recognition error:', event.error);\n        isRecording = false;\n        startButton.disabled = false;\n        if (silenceTimer) {\n            clearTimeout(silenceTimer);\n            silenceTimer = null;\n        }\n    };\n\n    // Handle when recognition ends\n    recognition.onend = () => {\n        if (isRecording) {\n            // If it ended but we're still supposed to be recording, restart it\n            recognition.start();\n        }\n    };\n}",
      "status": "[5]<br><span style=\"font-size:8px\">1ms<span></span></span>",
      "output": "() =&gt; {\n        if (isRecording) {\n            // If it ended but we're still supposed to be recording, restart it\n            recognition.start();\n        }\n    } <br>",
      "type": "code"
    },
    {
      "code": "//> html\n<button onclick=\"stopAudio()\" id=\"stop-btn\" style = 'width: 100%'>Stop</button>",
      "status": "[6]<br><span style=\"font-size:8px\">0ms<span></span></span>",
      "output": "\n<button onclick=\"stopAudio()\" id=\"stop-btn\" style=\"width: 100%\">Stop</button> <br>",
      "type": "code"
    }
  ],
  "source": "https://github.com/gopi-suvanam/scribbler",
  "run_on_load": true
}